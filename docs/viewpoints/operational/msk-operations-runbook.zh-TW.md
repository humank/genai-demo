# MSK Operations Runbook

**æ–‡æª”ç‰ˆæœ¬**: 2.0  
**æœ€å¾Œæ›´æ–°**: 2025å¹´9æœˆ24æ—¥ ä¸‹åˆ10:20 (å°åŒ—æ™‚é–“)  
**è² è²¬åœ˜éšŠ**: é‹ç‡Ÿåœ˜éšŠ + SRE åœ˜éšŠ

## ğŸ“‹ æ¦‚è¿°

æœ¬é‹ç‡Ÿæ‰‹å†Šæä¾› MSK (Amazon Managed Streaming for Apache Kafka) é›†ç¾¤çš„å®Œæ•´é‹ç‡ŸæŒ‡å—ï¼ŒåŒ…æ‹¬äº‹ä»¶éŸ¿æ‡‰ç¨‹åºã€å‡ç´šè·¯å¾‘ã€ç›£æ§ç¨‹åºã€å®¹é‡è¦åŠƒå’Œæ•…éšœæ’é™¤æŒ‡å—ã€‚

## ğŸš¨ äº‹ä»¶éŸ¿æ‡‰ç¨‹åº

### ç·Šæ€¥äº‹ä»¶åˆ†ç´š

#### P0 - ç·Šæ€¥ (Emergency) ğŸ”´
**å½±éŸ¿**: æœå‹™å®Œå…¨ä¸å¯ç”¨ï¼Œå½±éŸ¿æ‰€æœ‰ç”¨æˆ¶  
**éŸ¿æ‡‰æ™‚é–“**: ç«‹å³ (< 5 åˆ†é˜)  
**é€šçŸ¥æ–¹å¼**: é›»è©± + SMS + PagerDuty

**è§¸ç™¼æ¢ä»¶**:
- MSK é›†ç¾¤å®Œå…¨é›¢ç·š
- æ‰€æœ‰åˆ†å€é›¢ç·š (OfflinePartitionsCount > 0)
- ç„¡æ´»èºæ§åˆ¶å™¨ (ActiveControllerCount = 0)
- è³‡æ–™éºå¤±äº‹ä»¶

**éŸ¿æ‡‰ç¨‹åº**:
```bash
# 1. ç«‹å³è©•ä¼°å½±éŸ¿ç¯„åœ
kubectl get pods -n kafka-system
aws kafka describe-cluster --cluster-arn $MSK_CLUSTER_ARN

# 2. æª¢æŸ¥é›†ç¾¤ç‹€æ…‹
aws kafka list-clusters --query 'ClusterInfoList[0].State'

# 3. å¦‚æœé›†ç¾¤ç‹€æ…‹ç•°å¸¸ï¼Œå•Ÿå‹•ç½é›£æ¢å¾©
./scripts/disaster-recovery/initiate-failover.sh

# 4. é€šçŸ¥åˆ©å®³é—œä¿‚äºº
./scripts/notifications/send-emergency-alert.sh "MSK P0 Event"
```

#### P1 - åš´é‡ (Critical) ğŸŸ 
**å½±éŸ¿**: æœå‹™åŠŸèƒ½å—é™ï¼Œéƒ¨åˆ†ç”¨æˆ¶å—å½±éŸ¿  
**éŸ¿æ‡‰æ™‚é–“**: 15 åˆ†é˜å…§  
**é€šçŸ¥æ–¹å¼**: PagerDuty + Slack

**è§¸ç™¼æ¢ä»¶**:
- æ¶ˆè²»è€…å»¶é² > 5 åˆ†é˜ (EstimatedMaxTimeLag > 300000ms)
- æœªè¤‡è£½åˆ†å€ > 0 (UnderReplicatedPartitions > 0)
- ç”Ÿç”¢è€…éŒ¯èª¤ç‡ > 1%
- Broker CPU > 90%

**éŸ¿æ‡‰ç¨‹åº**:
```bash
# 1. æª¢æŸ¥æ¶ˆè²»è€…å»¶é²
aws kafka describe-cluster --cluster-arn $MSK_CLUSTER_ARN \
  --query 'ClusterInfo.CurrentBrokerSoftwareInfo'

# 2. åˆ†ææ¶ˆè²»è€…ç¾¤çµ„ç‹€æ…‹
kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --all-groups

# 3. æª¢æŸ¥åˆ†å€åˆ†ä½ˆ
kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --under-replicated-partitions

# 4. å¦‚éœ€è¦ï¼Œè§¸ç™¼è‡ªå‹•æ“´å±•
./scripts/scaling/auto-scale-consumers.sh
```

#### P2 - è­¦å‘Š (Warning) ğŸŸ¡
**å½±éŸ¿**: æ•ˆèƒ½ä¸‹é™ï¼Œç„¡ç”¨æˆ¶å½±éŸ¿  
**éŸ¿æ‡‰æ™‚é–“**: 1 å°æ™‚å…§  
**é€šçŸ¥æ–¹å¼**: Slack + Email

**è§¸ç™¼æ¢ä»¶**:
- æ¶ˆè²»è€…å»¶é² > 1 åˆ†é˜
- Broker CPU > 70%
- ç£ç¢Ÿä½¿ç”¨ç‡ > 80%
- ç”Ÿç”¢è€…éŒ¯èª¤ç‡ > 0.1%

### å‡ç´šè·¯å¾‘å’Œç¨‹åº

#### äº‹ä»¶å‡ç´šçŸ©é™£
```
æ™‚é–“ç¶“é    P0 â†’ P1 â†’ P2
15 åˆ†é˜     è‡ªå‹•å‡ç´šåˆ° P1
30 åˆ†é˜     è‡ªå‹•å‡ç´šåˆ° P0
1 å°æ™‚      å‡ç´šåˆ°ç®¡ç†å±¤
2 å°æ™‚      CEO/CTO é€šçŸ¥
```

#### å‡ç´šè¯çµ¡äºº
```yaml
Primary On-Call:
  - SRE Team Lead: +886-xxx-xxx-xxx
  - Platform Engineer: +886-xxx-xxx-xxx

Secondary On-Call:
  - Engineering Manager: +886-xxx-xxx-xxx
  - Architecture Lead: +886-xxx-xxx-xxx

Executive Escalation:
  - VP Engineering: +886-xxx-xxx-xxx
  - CTO: +886-xxx-xxx-xxx
```

## ğŸ“Š ç›£æ§ç¨‹åº

### ç›£æ§å„€è¡¨æ¿æª¢æŸ¥æ¸…å–®

#### æ¯æ—¥æª¢æŸ¥ (Daily Health Check)
**åŸ·è¡Œæ™‚é–“**: æ¯æ—¥ä¸Šåˆ 9:00  
**è² è²¬äºº**: å€¼ç­ SRE

```bash
#!/bin/bash
# daily-msk-health-check.sh

echo "=== MSK Daily Health Check $(date) ==="

# 1. é›†ç¾¤æ•´é«”å¥åº·
echo "1. Cluster Health:"
aws kafka describe-cluster --cluster-arn $MSK_CLUSTER_ARN \
  --query 'ClusterInfo.State'

# 2. Broker ç‹€æ…‹
echo "2. Broker Status:"
aws kafka list-nodes --cluster-arn $MSK_CLUSTER_ARN \
  --query 'NodeInfoList[*].[NodeARN,BrokerNodeInfo.BrokerId,BrokerNodeInfo.ClientSubnet]'

# 3. ä¸»é¡Œå¥åº·æª¢æŸ¥
echo "3. Topic Health:"
kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --list | wc -l
echo "Total topics count"

# 4. æ¶ˆè²»è€…ç¾¤çµ„ç‹€æ…‹
echo "4. Consumer Groups:"
kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --list | wc -l
echo "Active consumer groups"

# 5. æ•ˆèƒ½æŒ‡æ¨™æª¢æŸ¥
echo "5. Performance Metrics:"
curl -s "http://localhost:8080/actuator/msk-health" | jq '.overall_healthy'

# 6. ç”Ÿæˆå ±å‘Š
echo "Daily health check completed at $(date)" >> /var/log/msk-health.log
```

#### æ¯é€±æª¢æŸ¥ (Weekly Review)
**åŸ·è¡Œæ™‚é–“**: æ¯é€±ä¸€ä¸Šåˆ 10:00  
**è² è²¬äºº**: å¹³å°åœ˜éšŠ

```bash
#!/bin/bash
# weekly-msk-review.sh

echo "=== MSK Weekly Review $(date) ==="

# 1. å®¹é‡è¶¨å‹¢åˆ†æ
echo "1. Capacity Trends:"
aws cloudwatch get-metric-statistics \
  --namespace AWS/Kafka \
  --metric-name KafkaDataLogsDiskUsed \
  --start-time $(date -d '7 days ago' -u +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 86400 \
  --statistics Average

# 2. æ•ˆèƒ½è¶¨å‹¢
echo "2. Performance Trends:"
aws cloudwatch get-metric-statistics \
  --namespace AWS/Kafka \
  --metric-name EstimatedMaxTimeLag \
  --start-time $(date -d '7 days ago' -u +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 3600 \
  --statistics Maximum

# 3. éŒ¯èª¤ç‡åˆ†æ
echo "3. Error Rate Analysis:"
aws cloudwatch get-metric-statistics \
  --namespace AWS/Kafka \
  --metric-name ProducerRequestErrors \
  --start-time $(date -d '7 days ago' -u +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 3600 \
  --statistics Sum

# 4. æˆæœ¬åˆ†æ
echo "4. Cost Analysis:"
aws ce get-cost-and-usage \
  --time-period Start=$(date -d '7 days ago' +%Y-%m-%d),End=$(date +%Y-%m-%d) \
  --granularity DAILY \
  --metrics BlendedCost \
  --group-by Type=DIMENSION,Key=SERVICE \
  --filter file://msk-cost-filter.json
```

### è­¦å ±é–¾å€¼é…ç½®

#### é—œéµæŒ‡æ¨™é–¾å€¼
```yaml
Critical Thresholds:
  offline_partitions: 0          # ä»»ä½•é›¢ç·šåˆ†å€éƒ½æ˜¯ç·Šæ€¥äº‹ä»¶
  under_replicated_partitions: 0 # ä»»ä½•æœªè¤‡è£½åˆ†å€éƒ½æ˜¯åš´é‡äº‹ä»¶
  consumer_lag_ms: 300000        # 5 åˆ†é˜å»¶é²ç‚ºåš´é‡äº‹ä»¶
  broker_cpu_percent: 90         # CPU 90% ç‚ºåš´é‡äº‹ä»¶
  broker_memory_percent: 90      # è¨˜æ†¶é«” 90% ç‚ºåš´é‡äº‹ä»¶
  disk_usage_percent: 85         # ç£ç¢Ÿ 85% ç‚ºåš´é‡äº‹ä»¶

Warning Thresholds:
  consumer_lag_ms: 60000         # 1 åˆ†é˜å»¶é²ç‚ºè­¦å‘Š
  broker_cpu_percent: 70         # CPU 70% ç‚ºè­¦å‘Š
  broker_memory_percent: 80      # è¨˜æ†¶é«” 80% ç‚ºè­¦å‘Š
  disk_usage_percent: 80         # ç£ç¢Ÿ 80% ç‚ºè­¦å‘Š
  producer_error_rate: 0.001     # 0.1% éŒ¯èª¤ç‡ç‚ºè­¦å‘Š
  network_io_percent: 60         # ç¶²è·¯ I/O 60% ç‚ºè­¦å‘Š
```

#### è­¦å ±éŸ¿æ‡‰å‹•ä½œ
```yaml
Alert Actions:
  Critical:
    - SNS Topic: msk-critical-alerts
    - PagerDuty: High Priority
    - Slack: #ops-critical
    - Auto-scaling: Enabled
    
  Warning:
    - SNS Topic: msk-warning-alerts
    - Slack: #ops-monitoring
    - Email: ops-team@company.com
    - Auto-remediation: Enabled
```

## ğŸ“ˆ å®¹é‡è¦åŠƒæŒ‡å—

### å®¹é‡ç›£æ§æŒ‡æ¨™

#### 1. å„²å­˜å®¹é‡è¦åŠƒ
```bash
#!/bin/bash
# storage-capacity-planning.sh

# ç•¶å‰å„²å­˜ä½¿ç”¨ç‡
CURRENT_USAGE=$(aws cloudwatch get-metric-statistics \
  --namespace AWS/Kafka \
  --metric-name KafkaDataLogsDiskUsed \
  --start-time $(date -d '1 hour ago' -u +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 3600 \
  --statistics Average \
  --query 'Datapoints[0].Average')

echo "Current storage usage: ${CURRENT_USAGE}%"

# é æ¸¬æœªä¾† 30 å¤©ä½¿ç”¨é‡
if (( $(echo "$CURRENT_USAGE > 70" | bc -l) )); then
  echo "WARNING: Storage usage > 70%, consider scaling up"
  
  # è¨ˆç®—é è¨ˆæ»¿è¼‰æ™‚é–“
  GROWTH_RATE=$(calculate_growth_rate.py --days 30)
  DAYS_TO_FULL=$(echo "scale=0; (100 - $CURRENT_USAGE) / $GROWTH_RATE" | bc)
  
  echo "Estimated days to full capacity: $DAYS_TO_FULL"
  
  if (( DAYS_TO_FULL < 30 )); then
    echo "CRITICAL: Need to scale storage within 30 days"
    ./scripts/scaling/request-storage-scaling.sh
  fi
fi
```

#### 2. è¨ˆç®—å®¹é‡è¦åŠƒ
```python
#!/usr/bin/env python3
# capacity-planning.py

import boto3
from datetime import datetime, timedelta
import numpy as np

def calculate_capacity_requirements():
    cloudwatch = boto3.client('cloudwatch')
    
    # ç²å–éå» 30 å¤©çš„æŒ‡æ¨™
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=30)
    
    metrics = {
        'throughput': get_metric_data('MessagesInPerSec', start_time, end_time),
        'storage': get_metric_data('KafkaDataLogsDiskUsed', start_time, end_time),
        'cpu': get_metric_data('CpuUser', start_time, end_time),
        'memory': get_metric_data('MemoryUsed', start_time, end_time)
    }
    
    # è¨ˆç®—è¶¨å‹¢å’Œé æ¸¬
    predictions = {}
    for metric_name, data in metrics.items():
        trend = calculate_trend(data)
        prediction = predict_future_usage(data, days=90)
        predictions[metric_name] = {
            'current': data[-1] if data else 0,
            'trend': trend,
            'predicted_90_days': prediction
        }
    
    # ç”Ÿæˆå®¹é‡å»ºè­°
    recommendations = generate_capacity_recommendations(predictions)
    
    return {
        'predictions': predictions,
        'recommendations': recommendations,
        'timestamp': datetime.utcnow().isoformat()
    }

def generate_capacity_recommendations(predictions):
    recommendations = []
    
    # å„²å­˜å»ºè­°
    if predictions['storage']['predicted_90_days'] > 80:
        recommendations.append({
            'type': 'storage_scaling',
            'priority': 'high',
            'action': 'Increase broker storage by 50%',
            'timeline': '30 days'
        })
    
    # CPU å»ºè­°
    if predictions['cpu']['predicted_90_days'] > 70:
        recommendations.append({
            'type': 'compute_scaling',
            'priority': 'medium',
            'action': 'Upgrade to larger instance type',
            'timeline': '60 days'
        })
    
    # ååé‡å»ºè­°
    if predictions['throughput']['trend'] > 0.1:  # 10% å¢é•·
        recommendations.append({
            'type': 'partition_scaling',
            'priority': 'medium',
            'action': 'Increase partition count for high-traffic topics',
            'timeline': '45 days'
        })
    
    return recommendations

if __name__ == "__main__":
    results = calculate_capacity_requirements()
    print(json.dumps(results, indent=2))
```

### æ“´å±•è§¸ç™¼å™¨

#### è‡ªå‹•æ“´å±•é…ç½®
```yaml
Auto Scaling Triggers:
  Broker Scaling:
    scale_up_cpu_threshold: 70%
    scale_up_memory_threshold: 80%
    scale_up_network_threshold: 60%
    cooldown_period: 300s
    
  Storage Scaling:
    scale_up_disk_threshold: 80%
    scale_increment: 100GB
    max_storage_per_broker: 1TB
    
  Partition Scaling:
    scale_up_lag_threshold: 10000
    scale_up_throughput_threshold: 80%
    partition_increment: 4
```

#### æ‰‹å‹•æ“´å±•ç¨‹åº
```bash
#!/bin/bash
# manual-scaling-procedure.sh

function scale_brokers() {
  local target_count=$1
  
  echo "Scaling MSK cluster to $target_count brokers..."
  
  # 1. æ›´æ–° CDK é…ç½®
  sed -i "s/numberOfBrokerNodes: [0-9]*/numberOfBrokerNodes: $target_count/" \
    infrastructure/src/stacks/msk-stack.ts
  
  # 2. éƒ¨ç½²æ›´æ–°
  cd infrastructure
  cdk deploy MSKStack --require-approval never
  
  # 3. ç­‰å¾…æ“´å±•å®Œæˆ
  aws kafka describe-cluster --cluster-arn $MSK_CLUSTER_ARN \
    --query 'ClusterInfo.NumberOfBrokerNodes'
  
  # 4. é©—è­‰é›†ç¾¤å¥åº·
  ./scripts/health-check/verify-cluster-health.sh
  
  echo "Broker scaling completed"
}

function scale_storage() {
  local new_size_gb=$1
  
  echo "Scaling storage to ${new_size_gb}GB per broker..."
  
  # 1. å‰µå»ºæ“´å±•è«‹æ±‚
  aws kafka update-broker-storage \
    --cluster-arn $MSK_CLUSTER_ARN \
    --target-broker-ebs-volume-info VolumeSize=$new_size_gb
  
  # 2. ç›£æ§æ“´å±•é€²åº¦
  while true; do
    status=$(aws kafka describe-cluster --cluster-arn $MSK_CLUSTER_ARN \
      --query 'ClusterInfo.State' --output text)
    
    if [ "$status" = "ACTIVE" ]; then
      echo "Storage scaling completed"
      break
    fi
    
    echo "Scaling in progress... Status: $status"
    sleep 30
  done
}
```

## ğŸ”§ æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è¦‹å•é¡Œè¨ºæ–·

#### 1. æ¶ˆè²»è€…å»¶é²å•é¡Œ
```bash
#!/bin/bash
# diagnose-consumer-lag.sh

echo "=== Consumer Lag Diagnosis ==="

# æª¢æŸ¥æ‰€æœ‰æ¶ˆè²»è€…ç¾¤çµ„
echo "1. Consumer Groups Overview:"
kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --all-groups | grep -E "(GROUP|LAG)"

# è­˜åˆ¥å»¶é²æœ€åš´é‡çš„ç¾¤çµ„
echo "2. Top Lagging Consumer Groups:"
kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --all-groups | sort -k5 -nr | head -10

# æª¢æŸ¥ç‰¹å®šç¾¤çµ„è©³ç´°è³‡è¨Š
read -p "Enter consumer group to analyze: " GROUP_ID
echo "3. Detailed Analysis for $GROUP_ID:"

kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --group $GROUP_ID

# æª¢æŸ¥æ¶ˆè²»è€…å¯¦ä¾‹
echo "4. Consumer Instances:"
kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --group $GROUP_ID --members

# åˆ†æåˆ†å€åˆ†ä½ˆ
echo "5. Partition Distribution:"
kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --group $GROUP_ID --members --verbose

# å»ºè­°è§£æ±ºæ–¹æ¡ˆ
echo "6. Recommendations:"
LAG=$(kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --group $GROUP_ID | awk '{sum += $5} END {print sum}')

if (( LAG > 10000 )); then
  echo "- HIGH LAG DETECTED: Consider scaling consumers"
  echo "- Check consumer processing logic for bottlenecks"
  echo "- Verify network connectivity and DNS resolution"
fi
```

#### 2. ç”Ÿç”¢è€…æ•ˆèƒ½å•é¡Œ
```bash
#!/bin/bash
# diagnose-producer-performance.sh

echo "=== Producer Performance Diagnosis ==="

# æª¢æŸ¥ç”Ÿç”¢è€…æŒ‡æ¨™
echo "1. Producer Metrics:"
curl -s "http://localhost:8080/actuator/msk-metrics" | \
  jq '.throughput'

# æª¢æŸ¥æ‰¹æ¬¡é…ç½®
echo "2. Producer Configuration:"
curl -s "http://localhost:8080/actuator/configprops" | \
  jq '.kafka.producer'

# åˆ†æéŒ¯èª¤æ¨¡å¼
echo "3. Error Analysis:"
curl -s "http://localhost:8080/actuator/msk-errors" | \
  jq '.error_stats'

# æª¢æŸ¥ç¶²è·¯å»¶é²
echo "4. Network Latency Test:"
for broker in $(echo $BOOTSTRAP_SERVERS | tr ',' ' '); do
  echo "Testing $broker:"
  nc -zv ${broker%:*} ${broker#*:}
done

# å»ºè­°å„ªåŒ–
echo "5. Optimization Recommendations:"
echo "- Check batch.size and linger.ms configuration"
echo "- Verify compression.type setting"
echo "- Monitor buffer.memory usage"
echo "- Check for DNS resolution issues"
```

#### 3. åˆ†å€ä¸å¹³è¡¡å•é¡Œ
```bash
#!/bin/bash
# diagnose-partition-imbalance.sh

echo "=== Partition Imbalance Diagnosis ==="

# æª¢æŸ¥ä¸»é¡Œåˆ†å€åˆ†ä½ˆ
echo "1. Topic Partition Distribution:"
kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe | grep -E "(Topic:|Leader:)" | \
  awk '/Leader:/ {print $2, $4}' | sort | uniq -c

# æª¢æŸ¥ Broker è² è¼‰åˆ†ä½ˆ
echo "2. Broker Load Distribution:"
kafka-log-dirs.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --describe --json | jq -r '.brokers[].logDirs[].partitions | length'

# åˆ†æåˆ†å€å¤§å°
echo "3. Partition Size Analysis:"
for topic in $(kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --list); do
  echo "Topic: $topic"
  kafka-log-dirs.sh --bootstrap-server $BOOTSTRAP_SERVERS \
    --topic-list $topic --describe --json | \
    jq -r '.brokers[].logDirs[].partitions[] | "\(.partition): \(.size) bytes"'
done

# é‡æ–°å¹³è¡¡å»ºè­°
echo "4. Rebalancing Recommendations:"
echo "- Use kafka-reassign-partitions.sh for manual rebalancing"
echo "- Consider using Cruise Control for automated rebalancing"
echo "- Monitor rebalancing impact on performance"
```

### æ•ˆèƒ½èª¿å„ªæŒ‡å—

#### 1. Broker èª¿å„ª
```bash
#!/bin/bash
# broker-tuning.sh

echo "=== Broker Performance Tuning ==="

# JVM èª¿å„ªå»ºè­°
echo "1. JVM Tuning Recommendations:"
cat << EOF
# Kafka Broker JVM Settings
export KAFKA_HEAP_OPTS="-Xmx6g -Xms6g"
export KAFKA_JVM_PERFORMANCE_OPTS="-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true"
EOF

# OS èª¿å„ªå»ºè­°
echo "2. OS Tuning Recommendations:"
cat << EOF
# File descriptor limits
echo "* soft nofile 100000" >> /etc/security/limits.conf
echo "* hard nofile 100000" >> /etc/security/limits.conf

# VM settings
echo "vm.swappiness=1" >> /etc/sysctl.conf
echo "vm.dirty_background_ratio=5" >> /etc/sysctl.conf
echo "vm.dirty_ratio=60" >> /etc/sysctl.conf
echo "vm.dirty_expire_centisecs=12000" >> /etc/sysctl.conf
EOF

# ç¶²è·¯èª¿å„ª
echo "3. Network Tuning:"
cat << EOF
# TCP settings
echo "net.core.wmem_default = 131072" >> /etc/sysctl.conf
echo "net.core.rmem_default = 131072" >> /etc/sysctl.conf
echo "net.core.rmem_max = 16777216" >> /etc/sysctl.conf
echo "net.core.wmem_max = 16777216" >> /etc/sysctl.conf
EOF
```

#### 2. ä¸»é¡Œé…ç½®èª¿å„ª
```bash
#!/bin/bash
# topic-tuning.sh

function optimize_topic() {
  local topic=$1
  local throughput_requirement=$2
  
  echo "Optimizing topic: $topic for $throughput_requirement msgs/sec"
  
  # è¨ˆç®—å»ºè­°åˆ†å€æ•¸
  local partitions=$(( throughput_requirement / 1000 + 1 ))
  if (( partitions < 3 )); then
    partitions=3
  fi
  
  # æ›´æ–°ä¸»é¡Œé…ç½®
  kafka-configs.sh --bootstrap-server $BOOTSTRAP_SERVERS \
    --entity-type topics --entity-name $topic --alter \
    --add-config "segment.ms=604800000,retention.ms=604800000,compression.type=gzip"
  
  # å¢åŠ åˆ†å€ï¼ˆå¦‚æœéœ€è¦ï¼‰
  current_partitions=$(kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS \
    --describe --topic $topic | grep "PartitionCount" | awk '{print $4}')
  
  if (( partitions > current_partitions )); then
    kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS \
      --alter --topic $topic --partitions $partitions
    echo "Increased partitions from $current_partitions to $partitions"
  fi
}

# æ‰¹æ¬¡å„ªåŒ–æ‰€æœ‰æ¥­å‹™ä¸»é¡Œ
for topic in $(kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS \
  --list | grep "business-events"); do
  optimize_topic $topic 5000
done
```

## ğŸ”„ å‚™ä»½å’Œç½é›£æ¢å¾©

### å‚™ä»½ç­–ç•¥

#### 1. è‡ªå‹•å‚™ä»½é…ç½®
```yaml
Backup Configuration:
  type: "cross_region_replication"
  primary_region: "ap-northeast-1"
  backup_region: "ap-southeast-1"
  
  replication_settings:
    topics: "all_business_events"
    consumer_group_offsets: true
    acls: true
    
  retention:
    operational_backup: "7_days"
    compliance_backup: "90_days"
    disaster_recovery: "continuous"
```

#### 2. å‚™ä»½é©—è­‰ç¨‹åº
```bash
#!/bin/bash
# backup-verification.sh

echo "=== Backup Verification $(date) ==="

# 1. æª¢æŸ¥è·¨å€åŸŸè¤‡è£½ç‹€æ…‹
echo "1. Cross-Region Replication Status:"
aws kafka describe-replication \
  --replication-arn $REPLICATION_ARN \
  --query 'ReplicationInfo.ReplicationState'

# 2. é©—è­‰ä¸»é¡ŒåŒæ­¥
echo "2. Topic Synchronization:"
PRIMARY_TOPICS=$(kafka-topics.sh --bootstrap-server $PRIMARY_BOOTSTRAP \
  --list | sort)
BACKUP_TOPICS=$(kafka-topics.sh --bootstrap-server $BACKUP_BOOTSTRAP \
  --list | sort)

diff <(echo "$PRIMARY_TOPICS") <(echo "$BACKUP_TOPICS")

# 3. æª¢æŸ¥æ¶ˆè²»è€…ç¾¤çµ„åç§»é‡
echo "3. Consumer Group Offsets:"
for group in $(kafka-consumer-groups.sh --bootstrap-server $PRIMARY_BOOTSTRAP \
  --list); do
  
  PRIMARY_OFFSET=$(kafka-consumer-groups.sh \
    --bootstrap-server $PRIMARY_BOOTSTRAP \
    --describe --group $group | tail -n +3 | awk '{sum += $3} END {print sum}')
  
  BACKUP_OFFSET=$(kafka-consumer-groups.sh \
    --bootstrap-server $BACKUP_BOOTSTRAP \
    --describe --group $group | tail -n +3 | awk '{sum += $3} END {print sum}')
  
  LAG=$((PRIMARY_OFFSET - BACKUP_OFFSET))
  echo "Group $group: Primary=$PRIMARY_OFFSET, Backup=$BACKUP_OFFSET, Lag=$LAG"
  
  if (( LAG > 1000 )); then
    echo "WARNING: High replication lag for group $group"
  fi
done
```

### ç½é›£æ¢å¾©ç¨‹åº

#### RTO/RPO ç›®æ¨™
```yaml
Recovery Objectives:
  RTO: "< 5 minutes"    # Recovery Time Objective
  RPO: "< 1 minute"     # Recovery Point Objective
  
Service Level Targets:
  availability: "99.9%"
  data_loss_tolerance: "0%"
  max_downtime_per_month: "43.2 minutes"
```

#### ç½é›£æ¢å¾©åŸ·è¡Œç¨‹åº
```bash
#!/bin/bash
# disaster-recovery-execution.sh

function execute_disaster_recovery() {
  local scenario=$1  # "primary_failure" | "region_failure" | "complete_failure"
  
  echo "=== Executing Disaster Recovery: $scenario ==="
  
  case $scenario in
    "primary_failure")
      execute_primary_cluster_failover
      ;;
    "region_failure")
      execute_cross_region_failover
      ;;
    "complete_failure")
      execute_complete_rebuild
      ;;
    *)
      echo "Unknown disaster scenario: $scenario"
      exit 1
      ;;
  esac
}

function execute_primary_cluster_failover() {
  echo "1. Stopping primary cluster traffic..."
  
  # æ›´æ–° DNS è¨˜éŒ„æŒ‡å‘å‚™ç”¨é›†ç¾¤
  aws route53 change-resource-record-sets \
    --hosted-zone-id $HOSTED_ZONE_ID \
    --change-batch file://failover-dns-change.json
  
  # æ›´æ–°æ‡‰ç”¨ç¨‹å¼é…ç½®
  kubectl patch configmap kafka-config \
    --patch '{"data":{"bootstrap.servers":"'$BACKUP_BOOTSTRAP_SERVERS'"}}'
  
  # é‡å•Ÿæ‡‰ç”¨ç¨‹å¼ Pod
  kubectl rollout restart deployment/genai-demo-app
  
  echo "2. Verifying failover..."
  sleep 30
  
  # é©—è­‰æ‡‰ç”¨ç¨‹å¼é€£æ¥åˆ°å‚™ç”¨é›†ç¾¤
  kubectl logs -l app=genai-demo-app | grep "Connected to backup cluster"
  
  echo "Primary cluster failover completed"
}

function execute_cross_region_failover() {
  echo "1. Activating backup region..."
  
  # åœ¨å‚™ç”¨å€åŸŸéƒ¨ç½²å®Œæ•´åŸºç¤è¨­æ–½
  cd infrastructure
  AWS_REGION=$BACKUP_REGION cdk deploy --all
  
  # æ›´æ–°å…¨åŸŸè² è¼‰å¹³è¡¡å™¨
  aws globalaccelerator update-listener \
    --listener-arn $LISTENER_ARN \
    --port-ranges FromPort=443,ToPort=443,Protocol=TCP
  
  echo "2. Migrating traffic..."
  
  # é€æ­¥å°‡æµé‡åˆ‡æ›åˆ°å‚™ç”¨å€åŸŸ
  for weight in 25 50 75 100; do
    aws route53 change-resource-record-sets \
      --hosted-zone-id $HOSTED_ZONE_ID \
      --change-batch file://traffic-shift-${weight}.json
    
    echo "Traffic shifted to ${weight}% backup region"
    sleep 60
    
    # ç›£æ§éŒ¯èª¤ç‡
    error_rate=$(get_error_rate.sh)
    if (( $(echo "$error_rate > 0.01" | bc -l) )); then
      echo "High error rate detected, rolling back..."
      rollback_traffic_shift.sh
      exit 1
    fi
  done
  
  echo "Cross-region failover completed"
}
```

## ğŸ“‹ ç¶­è­·ç¨‹åº

### å®šæœŸç¶­è­·ä»»å‹™

#### æ¯æ—¥ç¶­è­· (Daily Maintenance)
```bash
#!/bin/bash
# daily-maintenance.sh

echo "=== Daily MSK Maintenance $(date) ==="

# 1. å¥åº·æª¢æŸ¥
./scripts/health-check/daily-health-check.sh

# 2. æ—¥èªŒè¼ªè½‰
find /var/log/kafka -name "*.log" -mtime +7 -delete

# 3. æŒ‡æ¨™æ”¶é›†
./scripts/monitoring/collect-daily-metrics.sh

# 4. å‚™ä»½é©—è­‰
./scripts/backup/verify-backup-status.sh

# 5. å®¹é‡æª¢æŸ¥
./scripts/capacity/check-capacity-usage.sh

echo "Daily maintenance completed"
```

#### æ¯é€±ç¶­è­· (Weekly Maintenance)
```bash
#!/bin/bash
# weekly-maintenance.sh

echo "=== Weekly MSK Maintenance $(date) ==="

# 1. æ•ˆèƒ½åˆ†æ
./scripts/performance/weekly-performance-analysis.sh

# 2. å®¹é‡è¦åŠƒ
./scripts/capacity/weekly-capacity-planning.sh

# 3. å®‰å…¨æƒæ
./scripts/security/weekly-security-scan.sh

# 4. é…ç½®å¯©æ ¸
./scripts/audit/weekly-config-audit.sh

# 5. æ–‡æª”æ›´æ–°
./scripts/documentation/update-runbook.sh

echo "Weekly maintenance completed"
```

#### æ¯æœˆç¶­è­· (Monthly Maintenance)
```bash
#!/bin/bash
# monthly-maintenance.sh

echo "=== Monthly MSK Maintenance $(date) ==="

# 1. ç½é›£æ¢å¾©æ¸¬è©¦
./scripts/dr/monthly-dr-test.sh

# 2. æ•ˆèƒ½åŸºæº–æ¸¬è©¦
./scripts/performance/monthly-benchmark.sh

# 3. æˆæœ¬å„ªåŒ–åˆ†æ
./scripts/cost/monthly-cost-analysis.sh

# 4. å®‰å…¨åˆè¦æª¢æŸ¥
./scripts/compliance/monthly-compliance-check.sh

# 5. æ¶æ§‹å¯©æ ¸
./scripts/architecture/monthly-architecture-review.sh

echo "Monthly maintenance completed"
```

### ç¶­è­·çª—å£ç®¡ç†

#### ç¶­è­·çª—å£æ’ç¨‹
```yaml
Maintenance Windows:
  daily:
    time: "02:00-04:00 UTC"
    duration: "2 hours"
    impact: "minimal"
    
  weekly:
    time: "Sunday 01:00-05:00 UTC"
    duration: "4 hours"
    impact: "low"
    
  monthly:
    time: "First Sunday 00:00-06:00 UTC"
    duration: "6 hours"
    impact: "medium"
    
  emergency:
    time: "as_needed"
    duration: "variable"
    impact: "high"
```

#### ç¶­è­·é€šçŸ¥ç¨‹åº
```bash
#!/bin/bash
# maintenance-notification.sh

function send_maintenance_notification() {
  local type=$1      # "scheduled" | "emergency"
  local start_time=$2
  local duration=$3
  local impact=$4
  
  # æº–å‚™é€šçŸ¥å…§å®¹
  cat << EOF > maintenance-notice.json
{
  "type": "$type",
  "start_time": "$start_time",
  "duration": "$duration",
  "impact": "$impact",
  "services_affected": ["MSK Cluster", "Event Processing", "Real-time Analytics"],
  "contact": "ops-team@company.com"
}
EOF

  # ç™¼é€é€šçŸ¥
  case $type in
    "scheduled")
      # æå‰ 24 å°æ™‚é€šçŸ¥
      aws sns publish \
        --topic-arn $MAINTENANCE_TOPIC_ARN \
        --message file://maintenance-notice.json \
        --subject "Scheduled MSK Maintenance - $start_time"
      ;;
    "emergency")
      # ç«‹å³é€šçŸ¥
      aws sns publish \
        --topic-arn $EMERGENCY_TOPIC_ARN \
        --message file://maintenance-notice.json \
        --subject "Emergency MSK Maintenance - Starting Now"
      ;;
  esac
}
```

## ğŸ“ è¯çµ¡è³‡è¨Šå’Œå‡ç´šè·¯å¾‘

### åœ˜éšŠè¯çµ¡è³‡è¨Š
```yaml
Primary Contacts:
  SRE Team Lead:
    name: "å¼µå°æ˜"
    phone: "+886-912-345-678"
    email: "sre-lead@company.com"
    slack: "@sre-lead"
    
  Platform Engineer:
    name: "æå°è¯"
    phone: "+886-987-654-321"
    email: "platform-eng@company.com"
    slack: "@platform-eng"

Secondary Contacts:
  Engineering Manager:
    name: "ç‹å¤§æ˜"
    phone: "+886-955-123-456"
    email: "eng-manager@company.com"
    
  Architecture Lead:
    name: "é™³å°ç¾"
    phone: "+886-933-789-012"
    email: "arch-lead@company.com"

Executive Escalation:
  VP Engineering:
    name: "æ—ç¸½ç›£"
    phone: "+886-911-111-111"
    email: "vp-eng@company.com"
    
  CTO:
    name: "é»ƒæŠ€è¡“é•·"
    phone: "+886-922-222-222"
    email: "cto@company.com"
```

### å¤–éƒ¨ä¾›æ‡‰å•†è¯çµ¡
```yaml
AWS Support:
  support_level: "Enterprise"
  case_priority: "Critical"
  phone: "+1-206-266-4064"
  web: "https://console.aws.amazon.com/support/"
  
Kafka Consulting:
  vendor: "Confluent Professional Services"
  contact: "support@confluent.io"
  phone: "+1-855-899-0121"
  
Monitoring Vendor:
  vendor: "Datadog"
  contact: "support@datadoghq.com"
  phone: "+1-866-329-4466"
```

---

**æ–‡æª”ç¶­è­·**: æœ¬é‹ç‡Ÿæ‰‹å†Šæ¯æœˆæ›´æ–°ä¸€æ¬¡  
**ä¸‹æ¬¡å¯©æ ¸**: 2025å¹´10æœˆ24æ—¥  
**ç·Šæ€¥è¯çµ¡**: ops-team@company.com | +886-911-MSK-OPS (911-675-677)
