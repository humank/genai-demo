#!/usr/bin/env python3
"""
åˆ†æ reports-summaries ç›®éŒ„ä¸­å¯ä»¥åˆªé™¤çš„éæ™‚å ±å‘Š
"""

import os
import re
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict

class ReportsCleanupAnalyzer:
    def __init__(self):
        self.reports_dir = Path("reports-summaries")
        self.backup_dir = Path("reports-summaries/.cleanup-backup")
        
        # å¯ä»¥åˆªé™¤çš„å ±å‘Šé¡å‹
        self.deletable_patterns = {
            # é‡è¤‡çš„å ±å‘Š (ä¿ç•™æœ€æ–°ç‰ˆæœ¬)
            "duplicates": [
                r"(.+)_1\.md$",  # å¸¶ _1 å¾Œç¶´çš„é‡è¤‡æ–‡ä»¶
                r"(.+)-\d{8}_\d{6}\.md$",  # å¸¶æ™‚é–“æˆ³çš„é‡è¤‡æ–‡ä»¶
            ],
            
            # éæ™‚çš„è‡¨æ™‚å ±å‘Š
            "temporary": [
                r".*-temp.*\.md$",
                r".*-draft.*\.md$", 
                r".*-wip.*\.md$",
                r".*-test.*\.md$",
            ],
            
            # å®Œæˆçš„é·ç§»å’Œä¿®å¾©å ±å‘Š (ä¿ç•™æœ€çµ‚å ±å‘Š)
            "completed_migrations": [
                r".*migration.*report.*\.md$",
                r".*fix.*report.*\.md$",
                r".*cleanup.*report.*\.md$",
                r".*completion.*report.*\.md$",
            ],
            
            # éæ™‚çš„åˆ†æå ±å‘Š (è¶…é30å¤©)
            "outdated_analysis": [
                r".*analysis.*\d{8}.*\.md$",
                r".*quality.*\d{8}.*\.md$",
                r".*validation.*\d{8}.*\.md$",
            ],
        }
        
        # å¿…é ˆä¿ç•™çš„é‡è¦å ±å‘Š
        self.keep_patterns = [
            r"README\.md$",
            r".*FINAL.*REPORT\.md$",
            r".*SUMMARY\.md$",
            r".*COMPLETION.*REPORT\.md$",
            r"SCRIPTS_CLEANUP_REPORT\.md$",
        ]
        
        # æŒ‰é¡åˆ¥åˆ†æçµæœ
        self.analysis_results = defaultdict(list)
        
    def is_important_report(self, file_path):
        """æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¦å ±å‘Š"""
        filename = file_path.name
        for pattern in self.keep_patterns:
            if re.match(pattern, filename, re.IGNORECASE):
                return True
        return False
    
    def get_file_age_days(self, file_path):
        """ç²å–æ–‡ä»¶å¹´é½¡ï¼ˆå¤©æ•¸ï¼‰"""
        try:
            mtime = file_path.stat().st_mtime
            file_date = datetime.fromtimestamp(mtime)
            return (datetime.now() - file_date).days
        except:
            return 0
    
    def find_duplicates(self):
        """æ‰¾å‡ºé‡è¤‡çš„å ±å‘Šæ–‡ä»¶"""
        duplicates = []
        
        for root, dirs, files in os.walk(self.reports_dir):
            root_path = Path(root)
            
            # æŒ‰åŸºæœ¬åç¨±åˆ†çµ„æ–‡ä»¶
            base_names = defaultdict(list)
            
            for file in files:
                if not file.endswith('.md'):
                    continue
                    
                file_path = root_path / file
                
                # æå–åŸºæœ¬åç¨± (å»é™¤ç‰ˆæœ¬è™Ÿå’Œæ™‚é–“æˆ³)
                base_name = file
                base_name = re.sub(r'_\d+\.md$', '.md', base_name)  # ç§»é™¤ _1, _2 ç­‰
                base_name = re.sub(r'-\d{8}_\d{6}\.md$', '.md', base_name)  # ç§»é™¤æ™‚é–“æˆ³
                base_name = re.sub(r'-\d{8}\.md$', '.md', base_name)  # ç§»é™¤æ—¥æœŸ
                
                base_names[base_name].append(file_path)
            
            # æ‰¾å‡ºæœ‰å¤šå€‹ç‰ˆæœ¬çš„æ–‡ä»¶
            for base_name, file_list in base_names.items():
                if len(file_list) > 1:
                    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œä¿ç•™æœ€æ–°çš„
                    file_list.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                    
                    # é™¤äº†æœ€æ–°çš„ï¼Œå…¶ä»–éƒ½æ˜¯é‡è¤‡çš„
                    for duplicate in file_list[1:]:
                        if not self.is_important_report(duplicate):
                            duplicates.append({
                                'file': duplicate,
                                'reason': f'Duplicate of {file_list[0].name}',
                                'category': 'duplicates',
                                'age_days': self.get_file_age_days(duplicate)
                            })
        
        return duplicates
    
    def find_outdated_reports(self):
        """æ‰¾å‡ºéæ™‚çš„å ±å‘Š"""
        outdated = []
        cutoff_date = datetime.now() - timedelta(days=30)
        
        for root, dirs, files in os.walk(self.reports_dir):
            root_path = Path(root)
            
            for file in files:
                if not file.endswith('.md'):
                    continue
                    
                file_path = root_path / file
                
                if self.is_important_report(file_path):
                    continue
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºéæ™‚çš„åˆ†æå ±å‘Š
                age_days = self.get_file_age_days(file_path)
                
                # ç‰¹å®šé¡å‹çš„éæ™‚å ±å‘Š
                if any(re.search(pattern, file, re.IGNORECASE) for pattern in [
                    r'quality.*\d{8}',
                    r'analysis.*\d{8}', 
                    r'validation.*\d{8}',
                    r'content-duplication.*\d{8}',
                    r'outdated-content.*\d{8}',
                ]):
                    if age_days > 7:  # è¶…é7å¤©çš„åˆ†æå ±å‘Š
                        outdated.append({
                            'file': file_path,
                            'reason': f'Outdated analysis report ({age_days} days old)',
                            'category': 'outdated_analysis',
                            'age_days': age_days
                        })
        
        return outdated
    
    def find_completed_tasks(self):
        """æ‰¾å‡ºå·²å®Œæˆä»»å‹™çš„å ±å‘Š"""
        completed = []
        
        # å·²å®Œæˆçš„é·ç§»å’Œä¿®å¾©ä»»å‹™
        completed_keywords = [
            'mermaid.*complete',
            'migration.*complete', 
            'fix.*complete',
            'cleanup.*complete',
            'implementation.*complete',
            '.*final.*report',
        ]
        
        for root, dirs, files in os.walk(self.reports_dir):
            root_path = Path(root)
            
            for file in files:
                if not file.endswith('.md'):
                    continue
                    
                file_path = root_path / file
                
                if self.is_important_report(file_path):
                    continue
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºå·²å®Œæˆä»»å‹™çš„ä¸­é–“å ±å‘Š
                for keyword in completed_keywords:
                    if re.search(keyword, file, re.IGNORECASE):
                        # å¦‚æœæœ‰å¤šå€‹ç›¸é—œå ±å‘Šï¼Œä¿ç•™æœ€çµ‚çš„
                        if not re.search(r'(final|summary|completion)', file, re.IGNORECASE):
                            completed.append({
                                'file': file_path,
                                'reason': f'Intermediate report for completed task',
                                'category': 'completed_tasks',
                                'age_days': self.get_file_age_days(file_path)
                            })
                        break
        
        return completed
    
    def analyze_quality_ux_reports(self):
        """ç‰¹åˆ¥åˆ†æ quality-ux ç›®éŒ„ä¸­çš„å¤§é‡é‡è¤‡å ±å‘Š"""
        quality_ux_dir = self.reports_dir / "quality-ux"
        if not quality_ux_dir.exists():
            return []
        
        deletable = []
        
        # æŒ‰é¡å‹åˆ†çµ„
        report_groups = defaultdict(list)
        
        for file_path in quality_ux_dir.glob("*.md"):
            filename = file_path.name
            
            # æå–å ±å‘Šé¡å‹å’Œæ—¥æœŸ
            if match := re.match(r'(.+)-(\d{8}_\d{6})\.(md|json)$', filename):
                report_type = match.group(1)
                timestamp = match.group(2)
                report_groups[report_type].append((file_path, timestamp))
        
        # å°æ¯å€‹é¡å‹ï¼Œåªä¿ç•™æœ€æ–°çš„2å€‹å ±å‘Š
        for report_type, files in report_groups.items():
            if len(files) > 2:
                # æŒ‰æ™‚é–“æˆ³æ’åº
                files.sort(key=lambda x: x[1], reverse=True)
                
                # ä¿ç•™æœ€æ–°çš„2å€‹ï¼Œåˆªé™¤å…¶ä»–çš„
                for file_path, timestamp in files[2:]:
                    deletable.append({
                        'file': file_path,
                        'reason': f'Old {report_type} report (keeping latest 2)',
                        'category': 'quality_ux_cleanup',
                        'age_days': self.get_file_age_days(file_path)
                    })
        
        return deletable
    
    def analyze_all_reports(self):
        """åˆ†ææ‰€æœ‰å ±å‘Š"""
        print("ğŸ” åˆ†æ reports-summaries ç›®éŒ„ä¸­çš„å ±å‘Š...")
        print("=" * 60)
        
        # æ”¶é›†æ‰€æœ‰å¯åˆªé™¤çš„å ±å‘Š
        all_deletable = []
        
        # 1. æ‰¾å‡ºé‡è¤‡æ–‡ä»¶
        duplicates = self.find_duplicates()
        all_deletable.extend(duplicates)
        self.analysis_results['duplicates'] = duplicates
        
        # 2. æ‰¾å‡ºéæ™‚å ±å‘Š
        outdated = self.find_outdated_reports()
        all_deletable.extend(outdated)
        self.analysis_results['outdated'] = outdated
        
        # 3. æ‰¾å‡ºå·²å®Œæˆä»»å‹™å ±å‘Š
        completed = self.find_completed_tasks()
        all_deletable.extend(completed)
        self.analysis_results['completed'] = completed
        
        # 4. ç‰¹åˆ¥è™•ç† quality-ux ç›®éŒ„
        quality_ux = self.analyze_quality_ux_reports()
        all_deletable.extend(quality_ux)
        self.analysis_results['quality_ux'] = quality_ux
        
        return all_deletable
    
    def print_analysis_results(self):
        """è¼¸å‡ºåˆ†æçµæœ"""
        total_files = sum(len(files) for files in self.analysis_results.values())
        
        print(f"ğŸ“Š åˆ†æçµæœç¸½è¦½:")
        print(f"   å¯åˆªé™¤å ±å‘Šç¸½æ•¸: {total_files}")
        print()
        
        for category, files in self.analysis_results.items():
            if files:
                print(f"ğŸ“ {category.replace('_', ' ').title()} ({len(files)} å€‹æ–‡ä»¶):")
                
                # æŒ‰ç›®éŒ„åˆ†çµ„é¡¯ç¤º
                by_dir = defaultdict(list)
                for item in files:
                    dir_name = item['file'].parent.name
                    by_dir[dir_name].append(item)
                
                for dir_name, dir_files in sorted(by_dir.items()):
                    print(f"   ğŸ“‚ {dir_name}/:")
                    for item in sorted(dir_files, key=lambda x: x['file'].name)[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                        age_info = f" ({item['age_days']}å¤©å‰)" if item['age_days'] > 0 else ""
                        print(f"     ğŸ—‘ï¸  {item['file'].name}{age_info}")
                    if len(dir_files) > 5:
                        print(f"     ... é‚„æœ‰ {len(dir_files) - 5} å€‹æ–‡ä»¶")
                print()
    
    def create_backup_and_delete(self, deletable_files, dry_run=True):
        """å‰µå»ºå‚™ä»½ä¸¦åˆªé™¤æ–‡ä»¶"""
        if not deletable_files:
            print("âœ… æ²’æœ‰éœ€è¦åˆªé™¤çš„æ–‡ä»¶")
            return
        
        print(f"{'ğŸ” æ¨¡æ“¬åˆªé™¤' if dry_run else 'ğŸ§¹ åŸ·è¡Œåˆªé™¤'}...")
        print("=" * 60)
        
        if not dry_run:
            # å‰µå»ºå‚™ä»½ç›®éŒ„
            self.backup_dir.mkdir(parents=True, exist_ok=True)
            print(f"âœ… å‰µå»ºå‚™ä»½ç›®éŒ„: {self.backup_dir}")
        
        deleted_count = 0
        
        for item in deletable_files:
            file_path = item['file']
            
            if dry_run:
                print(f"ğŸ” å°‡åˆªé™¤: {file_path.relative_to(self.reports_dir)} - {item['reason']}")
            else:
                try:
                    # å‰µå»ºç›¸å°æ‡‰çš„å‚™ä»½ç›®éŒ„çµæ§‹
                    relative_path = file_path.relative_to(self.reports_dir)
                    backup_path = self.backup_dir / relative_path
                    backup_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # å‚™ä»½æ–‡ä»¶
                    import shutil
                    shutil.copy2(file_path, backup_path)
                    
                    # åˆªé™¤åŸæ–‡ä»¶
                    file_path.unlink()
                    
                    print(f"ğŸ—‘ï¸  åˆªé™¤: {relative_path} (å·²å‚™ä»½)")
                    deleted_count += 1
                    
                except Exception as e:
                    print(f"âŒ åˆªé™¤å¤±æ•—: {relative_path} - {e}")
        
        if not dry_run:
            print(f"\nâœ… æ¸…ç†å®Œæˆï¼åˆªé™¤äº† {deleted_count} å€‹å ±å‘Š")
            print(f"ğŸ“ å‚™ä»½ä½ç½®: {self.backup_dir}")
        else:
            potential_deletes = len(deletable_files)
            print(f"\nğŸ” æ¨¡æ“¬å®Œæˆï¼å°‡åˆªé™¤ {potential_deletes} å€‹å ±å‘Š")

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆ†æå’Œæ¸…ç† reports-summaries ç›®éŒ„ä¸­çš„éæ™‚å ±å‘Š")
    parser.add_argument("--execute", action="store_true", help="åŸ·è¡Œå¯¦éš›æ¸…ç† (é è¨­ç‚ºæ¨¡æ“¬æ¨¡å¼)")
    parser.add_argument("--analyze-only", action="store_true", help="åªåˆ†æä¸æ¸…ç†")
    
    args = parser.parse_args()
    
    analyzer = ReportsCleanupAnalyzer()
    
    # åˆ†æå ±å‘Š
    deletable_files = analyzer.analyze_all_reports()
    
    # é¡¯ç¤ºåˆ†æçµæœ
    analyzer.print_analysis_results()
    
    if not args.analyze_only:
        # åŸ·è¡Œæ¸…ç†
        analyzer.create_backup_and_delete(deletable_files, dry_run=not args.execute)
        
        if not args.execute:
            print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --execute åƒæ•¸ä¾†åŸ·è¡Œå¯¦éš›æ¸…ç†")

if __name__ == "__main__":
    main()